{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839},{"sourceId":375253,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":310173,"modelId":330548}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, Subset, random_split\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:45.944590Z","iopub.execute_input":"2025-05-05T21:18:45.945413Z","iopub.status.idle":"2025-05-05T21:18:46.850806Z","shell.execute_reply.started":"2025-05-05T21:18:45.945382Z","shell.execute_reply":"2025-05-05T21:18:46.850254Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None, dropout_rate=0.1):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, dropout_rate=0.1):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels, dropout_rate=dropout_rate)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True, dropout_rate=0.1):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2, dropout_rate=dropout_rate)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels, dropout_rate=dropout_rate)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:46.852030Z","iopub.execute_input":"2025-05-05T21:18:46.852668Z","iopub.status.idle":"2025-05-05T21:18:46.862021Z","shell.execute_reply.started":"2025-05-05T21:18:46.852647Z","shell.execute_reply":"2025-05-05T21:18:46.861497Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=False, dropout_rate=0.1):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = (DoubleConv(n_channels, 64, dropout_rate=dropout_rate))\n        self.down1 = (Down(64, 128, dropout_rate=dropout_rate))\n        self.down2 = (Down(128, 256, dropout_rate=dropout_rate))\n        self.down3 = (Down(256, 512, dropout_rate=dropout_rate))\n        factor = 2 if bilinear else 1\n        self.down4 = (Down(512, 1024 // factor, dropout_rate=dropout_rate))\n        self.up1 = (Up(1024, 512 // factor, bilinear, dropout_rate=dropout_rate))\n        self.up2 = (Up(512, 256 // factor, bilinear, dropout_rate=dropout_rate))\n        self.up3 = (Up(256, 128 // factor, bilinear, dropout_rate=dropout_rate))\n        self.up4 = (Up(128, 64, bilinear, dropout_rate=dropout_rate))\n        self.outc = (OutConv(64, n_classes))\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:46.862780Z","iopub.execute_input":"2025-05-05T21:18:46.863031Z","iopub.status.idle":"2025-05-05T21:18:46.886202Z","shell.execute_reply.started":"2025-05-05T21:18:46.863010Z","shell.execute_reply":"2025-05-05T21:18:46.885675Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"PATCH_SIZE = (256, 256)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:46.887440Z","iopub.execute_input":"2025-05-05T21:18:46.887624Z","iopub.status.idle":"2025-05-05T21:18:46.910353Z","shell.execute_reply.started":"2025-05-05T21:18:46.887610Z","shell.execute_reply":"2025-05-05T21:18:46.909829Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"unet = UNet(1, 2, bilinear=False, dropout_rate=0.1).to(DEVICE)\n\nunet = nn.DataParallel(unet)\nunet.load_state_dict(torch.load(\"/kaggle/input/best_model/pytorch/default/1/best_model.pth\", map_location=torch.device(DEVICE)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:46.910903Z","iopub.execute_input":"2025-05-05T21:18:46.911248Z","iopub.status.idle":"2025-05-05T21:18:47.374645Z","shell.execute_reply.started":"2025-05-05T21:18:46.911221Z","shell.execute_reply":"2025-05-05T21:18:47.373882Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2631330684.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  unet.load_state_dict(torch.load(\"/kaggle/input/best_model/pytorch/default/1/best_model.pth\", map_location=torch.device(DEVICE)))\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def get_nodule(model, image):\n    model.eval()\n    image = transforms.Resize(PATCH_SIZE, antialias=True)(image)\n    image = transforms.ToTensor()(image)               # [1, H, W]\n\n    with torch.no_grad():\n        output = model(image.unsqueeze(0).to(DEVICE)).cpu()\n    pred_mask = torch.argmax(output, dim=1).float()\n    \n    mask_np = pred_mask.squeeze().numpy().astype(np.uint8)\n    kernel  = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    opened  = cv2.morphologyEx(mask_np, cv2.MORPH_OPEN, kernel)\n    dilated = cv2.morphologyEx(opened, cv2.MORPH_DILATE, kernel)\n    binary_mask = torch.from_numpy(dilated).unsqueeze(0)\n\n    coords = cv2.findNonZero(dilated)\n    if coords is None:  # If no coordinates are found (empty mask)\n        print(\"Warning: Skipping image due to no valid coordinates.\")\n        return None  # Skip this image\n        \n    x, y, w, h = cv2.boundingRect(coords)\n    cropped_img  = image[:, y : y+h, x : x+w]\n    cropped_mask = binary_mask[:, y : y+h, x : x+w]\n    \n    if cropped_img.shape[-2] == 0 or cropped_img.shape[-1] == 0:\n        print(\"Warning: Skipping image due to empty crop.\")\n        return None\n\n    batch_img  = cropped_img.unsqueeze(0)\n    batch_mask = cropped_mask.unsqueeze(0)\n    resized_img  = F.interpolate(batch_img, size=(300,300), mode='bilinear',    align_corners=False).squeeze(0)\n    resized_mask = F.interpolate(batch_mask, size=(300,300), mode='nearest').squeeze(0)\n    masked_image = resized_img * resized_mask\n    return masked_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:47.375356Z","iopub.execute_input":"2025-05-05T21:18:47.375561Z","iopub.status.idle":"2025-05-05T21:18:47.386049Z","shell.execute_reply.started":"2025-05-05T21:18:47.375545Z","shell.execute_reply":"2025-05-05T21:18:47.385513Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.utils import resample\n\ndf = pd.read_csv('/kaggle/input/data/Data_Entry_2017.csv')\n\nnodule_mask     = df['Finding Labels'].apply(lambda labels: 'Nodule' in labels.split('|'))\nother_mask      = ~nodule_mask\n\nnodule_df       = df[nodule_mask]\nsecond_df       = df[other_mask]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:47.386721Z","iopub.execute_input":"2025-05-05T21:18:47.386908Z","iopub.status.idle":"2025-05-05T21:18:47.607424Z","shell.execute_reply.started":"2025-05-05T21:18:47.386893Z","shell.execute_reply":"2025-05-05T21:18:47.606640Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport zipfile\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport pandas as pd\nimport zipfile\nfrom torchvision import transforms\nfrom PIL import Image\n\nsource_base_dir = '/kaggle/input/data/'\n\nOUT_BASE = \"/kaggle/working/processed_lungs\"\nos.makedirs(os.path.join(OUT_BASE, \"nodule\"),    exist_ok=True)\nos.makedirs(os.path.join(OUT_BASE, \"non_nodule\"), exist_ok=True)\n\nfolder_ranges = [\n    (1335, 6, 'images_001'),\n    (3923, 13, 'images_002'),\n    (6585, 6, 'images_003'),\n    (9232, 3, 'images_004'),\n    (11558, 7, 'images_005'),\n    (13774, 25, 'images_006'),\n    (16051, 9, 'images_007'),\n    (18387, 34, 'images_008'),\n    (20945, 49, 'images_009'),\n    (24717, 0, 'images_010'),\n    (28173, 2, 'images_011'),\n    (30805, 0, 'images_012')\n]\nrecords = []\nfor cohort_df, label_value in [(nodule_df, 1.0), (second_df, 0.0)]:\n    subfolder = \"nodule\" if label_value == 1.0 else \"non_nodule\"\n    out_dir   = os.path.join(OUT_BASE, subfolder)\n    \n    for _, row in cohort_df.iterrows():\n        image_filename = row['Image Index']\n        \n        base_name = os.path.splitext(image_filename)[0]\n        part1_str, part2_str = base_name.split('_')\n        part1, part2 = int(part1_str), int(part2_str)\n\n        source_folder = None\n        for f_part1, f_part2, f_name in folder_ranges:\n            if part1 < f_part1 or (part1 == f_part1 and part2 <= f_part2):\n                source_folder = f_name\n                break\n        if not source_folder:\n            continue\n\n        img_path = os.path.join(source_base_dir,\n                                source_folder,\n                                'images',\n                                image_filename)\n\n        image = Image.open(img_path).convert('L')\n        image = get_nodule(unet, image)\n        \n        if image is not None:\n            pil_img = transforms.ToPILImage()(image)\n            save_name = f\"{base_name}_lung.png\"\n            save_path = os.path.join(out_dir, save_name)\n            pil_img.save(save_path)\n            \n            records.append({\n                \"filename\": save_name,\n                \"label\":    int(label_value)\n            })\n        else:\n            print(\"Warning: Skipping image due to invalid or empty result from get_nodule.\")\n\n\ndf_out = pd.DataFrame(records)\ncsv_path = os.path.join(OUT_BASE, \"labels.csv\")\ndf_out.to_csv(csv_path, index=False)\nprint(f\"Saved {len(records)} processed lungs and labels.csv to {OUT_BASE}\")\n\nzip_path = \"/kaggle/working/processed_lungs.zip\"\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUT_BASE):\n        for fname in files:\n            full = os.path.join(root, fname)\n            # write with relative path inside the zip\n            rel  = os.path.relpath(full, os.path.dirname(OUT_BASE))\n            zf.write(full, rel)\n\nprint(f\"Created ZIP archive at {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T21:18:47.608342Z","iopub.execute_input":"2025-05-05T21:18:47.608980Z","iopub.status.idle":"2025-05-05T23:48:40.674632Z","shell.execute_reply.started":"2025-05-05T21:18:47.608959Z","shell.execute_reply":"2025-05-05T23:48:40.673836Z"}},"outputs":[{"name":"stdout","text":"Warning: Skipping image due to no valid coordinates.\nWarning: Skipping image due to invalid or empty result from get_nodule.\nWarning: Skipping image due to no valid coordinates.\nWarning: Skipping image due to invalid or empty result from get_nodule.\nWarning: Skipping image due to no valid coordinates.\nWarning: Skipping image due to invalid or empty result from get_nodule.\nWarning: Skipping image due to no valid coordinates.\nWarning: Skipping image due to invalid or empty result from get_nodule.\nSaved 112116 processed lungs and labels.csv to /kaggle/working/processed_lungs\nCreated ZIP archive at /kaggle/working/processed_lungs.zip\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(\"processed_lungs.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T23:58:30.366087Z","iopub.execute_input":"2025-05-05T23:58:30.366786Z","iopub.status.idle":"2025-05-05T23:58:30.371817Z","shell.execute_reply.started":"2025-05-05T23:58:30.366761Z","shell.execute_reply":"2025-05-05T23:58:30.371123Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/processed_lungs.zip","text/html":"<a href='processed_lungs.zip' target='_blank'>processed_lungs.zip</a><br>"},"metadata":{}}],"execution_count":19}]}
