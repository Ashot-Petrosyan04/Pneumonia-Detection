{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as F\nimport cv2\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import binary_dilation, maximum_filter, minimum_filter\nfrom torchvision.models.inception import InceptionAux","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/processed-lungs/processed_lungs/labels.csv')\n\nnodule_df = df[df['label'] == 1]\nsecond_df = df[df['label'] == 0]\n\nprint(\"we print nodule \")\nprint(nodule_df)\nprint(\"now second one\")\nprint(second_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_bbox = pd.read_csv('/kaggle/input/bbox-list-2017-csv/BBox_List_2017.csv')\n\ndf_bbox = df_bbox[df_bbox[\"Finding Label\"] == \"Nodule\"]\nprint(df_bbox)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 8\nLR = 0.001\nEPOCHS = 10\n\nmain_dest_dir = '/kaggle/working/'\nsource_base_dir = '/kaggle/input/processed-lungs'\n\nclass SegmentedChestXRayDataset(Dataset):\n    def __init__(self, input_size):\n        self.image_paths = []\n        self.labels = []\n        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                     \n        self.transform_positive = transforms.Compose([\n            transforms.Resize(input_size),\n            transforms.ToTensor(),\n            self.normalize\n        ])\n        \n        self.transform_negative = transforms.Compose([\n            transforms.Resize(input_size),\n            transforms.ToTensor(),\n            self.normalize\n        ])\n        \n        source_folder = 'processed_lungs/nodule'\n        for _, row in df_bbox.iterrows():\n            image_filename = row['Image Index']\n            img_path = os.path.join(source_base_dir,\n                                    source_folder,\n                                    image_filename)\n            root, ext = os.path.splitext(img_path)\n            new_path = f\"{root}_lung{ext}\"\n            self.image_paths.append(new_path)\n\n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        image = Image.open(img_path).convert('RGB')\n        \n        image = self.transform_positive(image)\n            \n        return image, img_path\n\n    def __len__(self):\n        return len(self.image_paths)\n        \n    def tackle_idxs(self, idxs):\n        image_paths_temp = []\n        labels_temp = []\n        \n        for i in idxs:\n            label = self.labels[i]\n            img_path = self.image_paths[i]\n            \n            image_paths_temp.append(img_path)\n            labels_temp.append(label)\n        \n        combined = list(zip(image_paths_temp, labels_temp))\n        random.shuffle(combined)\n        self.image_paths, self.labels = map(list, zip(*combined))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source_base_dir_data = '/kaggle/input/data/'\n\nfolder_ranges = [\n    (1335, 6, 'images_001'),\n    (3923, 13, 'images_002'),\n    (6585, 6, 'images_003'),\n    (9232, 3, 'images_004'),\n    (11558, 7, 'images_005'),\n    (13774, 25, 'images_006'),\n    (16051, 9, 'images_007'),\n    (18387, 34, 'images_008'),\n    (20945, 49, 'images_009'),\n    (24717, 0, 'images_010'),\n    (28173, 2, 'images_011'),\n    (30805, 0, 'images_012')\n]\n\nclass ChestXRayDataset(Dataset):\n    def __init__(self):\n        self.image_paths = []\n        self.labels = []\n        \n        for _, row in df_bbox.iterrows():\n            image_filename = row['Image Index']\n            x_cord = row['Bbox [x']\n            y_cord = row['y']\n            width_w = row['w']\n            height_h = row['h]']\n            base_name = os.path.splitext(image_filename)[0]\n            part1_str, part2_str = base_name.split('_')\n            part1, part2 = int(part1_str), int(part2_str)\n    \n            source_folder = None\n            for f_part1, f_part2, f_name in folder_ranges:\n                if part1 < f_part1 or (part1 == f_part1 and part2 <= f_part2):\n                    source_folder = f_name\n                    break\n            if not source_folder:\n                continue\n    \n            img_path = os.path.join(source_base_dir_data,\n                                    source_folder,\n                                    'images',\n                                    image_filename)\n    \n            self.image_paths.append(img_path)\n            self.labels.append([x_cord, y_cord, width_w, height_h])\n\n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        label = self.labels[index]\n        image = Image.open(img_path).convert('RGB')\n        \n        return label, img_path\n\n    def __len__(self):\n        return len(self.image_paths)\n        \n    def tackle_idxs(self, idxs):\n        image_paths_temp = []\n        labels_temp = []\n        \n        for i in idxs:\n            label = self.labels[i]\n            img_path = self.image_paths[i]\n            \n            image_paths_temp.append(img_path)\n            labels_temp.append(label)\n        \n        combined = list(zip(image_paths_temp, labels_temp))\n        random.shuffle(combined)\n        self.image_paths, self.labels = map(list, zip(*combined))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DenseNet121(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.densenet = torchvision.models.densenet121(weights=\"IMAGENET1K_V1\")\n\n        num_features = self.densenet.classifier.in_features\n        #self.densenet.classifier = nn.Linear(num_features, 1)\n        \n        self.densenet.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_features, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1)\n        )\n        \n        for param in self.densenet.parameters():\n            param.requires_grad = False\n\n        to_unfreeze = [\n            \"features.denseblock2\",\n            \"features.transition2\",\n            \"features.denseblock3\",\n            \"features.transition3\",\n            \"features.denseblock4\",\n            \"features.norm5\",\n            \"classifier\",\n        ]\n        \n        for name, param in self.densenet.named_parameters():\n            if any(name.startswith(layer) for layer in to_unfreeze):\n                param.requires_grad = True\n    \n    def forward(self, x):\n        return self.densenet(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DenseNet121GradCam(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n\n        self.densenetGradCam = model\n        self.features_conv = model.densenet.features[:11]\n        # print(self.densenetGradCam)\n        # print(self.features_conv)\n        self.batch_norm = model.densenet.features.norm5\n        self.avgpool_head = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.classifier = model.densenet.classifier\n        self.sigmoid = nn.Sigmoid()\n        self.gradients = None\n\n    def activation_hook(self, grad):\n        self.gradients = grad\n \n    def forward(self, x):\n        x = self.features_conv(x)\n        h = x.register_hook(self.activation_hook)\n        x = self.batch_norm(x)\n        x = self.avgpool_head(x)\n        x = self.flatten(x)\n        x = self.classifier(x)\n        \n        return x\n    \n    def get_activation_gradients(self):\n        return self.gradients\n    \n    def get_activations(self, x):\n        return self.features_conv(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Resnet34(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.resnet34 = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n\n        for param in self.resnet34.parameters():\n            param.requires_grad = False\n\n        to_unfreeze = ['layer3', 'layer4', 'fc']\n        for name, param in self.resnet34.named_parameters():\n            if any(name.startswith(layer) for layer in to_unfreeze):\n                param.requires_grad = True\n\n        num_ftrs = self.resnet34.fc.in_features\n        self.resnet34.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_ftrs, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, x):\n        return self.resnet34(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Resnet34GradCam(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n\n        self.resnetGradCam = model\n        self.features_conv = nn.Sequential(*list(model.resnet34.children())[:-2])\n\n        # print(self.resnetGradCam)\n        # print(self.features_conv)\n        \n        self.avgpool_head = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.classifier = model.resnet34.fc\n        self.sigmoid = nn.Sigmoid()\n        self.gradients = None\n\n    def activation_hook(self, grad):\n        self.gradients = grad\n \n    def forward(self, x):\n        x = self.features_conv(x)\n        h = x.register_hook(self.activation_hook)\n        \n        x = self.avgpool_head(x)\n        x = self.flatten(x)\n        x = self.classifier(x)\n        \n        return x\n    \n    def get_activation_gradients(self):\n        return self.gradients\n    \n    def get_activations(self, x):\n        return self.features_conv(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Inception_V3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.inception_v3 = torchvision.models.inception_v3(pretrained=True)\n        \n        num_features = self.inception_v3.fc.in_features\n        self.inception_v3.fc = nn.Linear(num_features, 1)\n        \n        if self.inception_v3.aux_logits:\n            num_features_aux = self.inception_v3.AuxLogits.fc.in_features\n            self.inception_v3.AuxLogits.fc = nn.Linear(num_features_aux, 1)\n\n        for name, param in self.inception_v3.named_parameters():\n            param.requires_grad = False\n                \n        for name, param in self.inception_v3.named_parameters():\n            if name.startswith('Mixed_7c') or name.startswith('fc') or name.startswith('AuxLogits.fc'):\n                param.requires_grad = True\n    \n    def forward(self, x):\n        return self.inception_v3(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Inception_V3GradCam(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n\n        self.inceptionGradCam = model\n        self.features_conv = nn.Sequential(\n            *[\n              m for m in model.inception_v3.children()\n              if not isinstance(m, InceptionAux)\n            ][:-3]\n        )\n\n        # print(self.inceptionGradCam)\n        # print(self.features_conv)\n        \n        self.avgpool_head = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.dropout = model.inception_v3.dropout\n        self.classifier = model.inception_v3.fc\n        self.sigmoid = nn.Sigmoid()\n        self.gradients = None\n\n    def activation_hook(self, grad):\n        self.gradients = grad\n \n    def forward(self, x):\n        x = self.features_conv(x)\n        h = x.register_hook(self.activation_hook)\n        x = self.avgpool_head(x)\n        x = self.flatten(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        \n        return x\n    \n    def get_activation_gradients(self):\n        return self.gradients\n    \n    def get_activations(self, x):\n        return self.features_conv(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    test_dataset_Inception_V3 = SegmentedChestXRayDataset(299)\n    test_loader_Inception_V3 = DataLoader(test_dataset_Inception_V3, batch_size=1, shuffle=False)\n    \n    test_dataset = SegmentedChestXRayDataset(224)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n    \n    test_dataset_bbox = ChestXRayDataset()\n    test_loader_bbox = DataLoader(test_dataset_bbox, batch_size=1, shuffle=False)\n    \n    model_DenseNet121 = DenseNet121().to(device)\n    model_DenseNet121.load_state_dict(torch.load(\"/kaggle/input/densenet121_final_model.pth/pytorch/default/1/DenseNet121_final_model.pth\", map_location=device))\n    model_DenseNet121.eval()\n    \n    gradcam_DenseNet121 = DenseNet121GradCam(model_DenseNet121)\n    gradcam_DenseNet121.eval()\n    \n    model_Resnet34 = Resnet34().to(device)\n    model_Resnet34.load_state_dict(torch.load(\"/kaggle/input/resnet34_final_model.pth/pytorch/default/1/Resnet34_final_model.pth\", map_location=device))\n    model_Resnet34.eval()\n    \n    gradcam_Resnet34 = Resnet34GradCam(model_Resnet34)\n    gradcam_Resnet34.eval()\n    \n    model_Inception_V3 = Inception_V3().to(device)\n    model_Inception_V3.load_state_dict(torch.load(\"/kaggle/input/inception_v3_final_model.pth/pytorch/default/1/Inception_V3_final_model.pth\", map_location=device))\n    model_Inception_V3.eval()\n    \n    gradcam_Inception_V3 = Inception_V3GradCam(model_Inception_V3)\n    gradcam_Inception_V3.eval()\n    \n    for idx, ((image_Inception_V3, img_path_Inception_V3), (image_Resnet34_DenseNet121, img_path_Resnet34_DenseNet121), (label_bbox, img_path_bbox)) in enumerate(zip(test_loader_Inception_V3, test_loader, test_loader_bbox)):\n        print(idx)\n        \n        image_Resnet34_DenseNet121 = image_Resnet34_DenseNet121.to(device)\n        image_Inception_V3 = image_Inception_V3.to(device)\n        image_Resnet34_DenseNet121.requires_grad = True\n        image_Inception_V3.requires_grad = True\n        \n        for name_of_gradcam, gradcam, image, img_path in [(\"gradcam_DenseNet121\", gradcam_DenseNet121, image_Resnet34_DenseNet121, img_path_Resnet34_DenseNet121), (\"gradcam_Resnet34\", gradcam_Resnet34, image_Resnet34_DenseNet121, img_path_Resnet34_DenseNet121), (\"gradcam_Inception_V3\", gradcam_Inception_V3, image_Inception_V3, img_path_Inception_V3)]:\n            gradcam.zero_grad()\n            logits = gradcam(image)\n            logits[0, 0].backward()\n    \n            activations = gradcam.get_activations(image).cpu().detach()\n            gradients = gradcam.get_activation_gradients().cpu()\n            pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n    \n            for i in range(activations.shape[1]):\n                activations[:, i, :, :] *= pooled_gradients[i]\n    \n            heatmap = torch.sum(activations, dim=1).squeeze()\n            heatmap = torch.clamp(heatmap, min=0)\n            heatmap /= torch.max(heatmap)\n            heatmap = heatmap.numpy()\n    \n            image_original = cv2.imread(img_path[0])\n            heatmap_resized = cv2.resize(heatmap, (image_original.shape[1], image_original.shape[0]))\n            heatmap_resized_uint8 = np.uint8(255 * heatmap_resized)\n            heatmap_color = cv2.applyColorMap(heatmap_resized_uint8, cv2.COLORMAP_JET)\n            superimposed_img = heatmap_color * 0.4 + image_original\n            superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n    \n            neighborhood_size = 15\n            threshold_val = 0.1\n    \n            heatmap_norm = heatmap_resized_uint8.copy()\n    \n            data_max = maximum_filter(heatmap_norm, size=neighborhood_size)\n            maxima = (heatmap_norm == data_max)\n            data_min = minimum_filter(heatmap_norm, size=neighborhood_size)\n            diff = ((data_max - data_min) > threshold_val)\n            maxima[diff == 0] = 0\n    \n            for _ in range(5):\n                maxima = binary_dilation(maxima)\n    \n            labeled, num_objects = ndimage.label(maxima)\n            centers = ndimage.center_of_mass(heatmap_norm, labeled, range(1, num_objects+1))\n    \n            scale_factor = image_original.shape[1] / 224\n            w_k = int(122 * scale_factor)\n            h_k = int(132 * scale_factor)\n            \n            for center in centers:\n                y, x = center\n                if heatmap_norm[int(y), int(x)] > np.max(heatmap_norm) * 0.9:\n                    left = int(max(x - w_k/2, 0))\n                    upper = int(max(y - h_k/2, 0))\n                    right = int(min(left + w_k, image_original.shape[1]))\n                    lower = int(min(upper + h_k, image_original.shape[0]))\n                    left = int(left)\n                    right = int(right)\n                    upper = int(upper)\n                    lower = int(lower)\n    \n                    cv2.rectangle(superimposed_img, (left, upper), (right, lower), (0, 255, 0), 2)\n    \n            cv2.imwrite(f\"{name_of_gradcam}_{idx}.jpg\", superimposed_img)\n\n        original_img_bbox = cv2.imread(img_path_bbox[0])\n        if original_img_bbox is not None:\n            x, y, w, h = label_bbox\n            x = int(x.item())\n            y = int(y.item())\n            w = int(w.item())\n            h = int(h.item())\n            print(x)\n            print(y)\n            print(w)\n            print(h)\n            cv2.rectangle(original_img_bbox, (x, y), (x + w, y + h), (0, 255, 0), 2)\n            \n            cv2.imwrite(f\"orig_{idx}.jpg\", original_img_bbox)\n        else:\n            print(f\"Failed to load image: {img_path_bbox[0]}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\nOUT_BASE = \"/kaggle/working/\"\nzip_path = os.path.join(OUT_BASE, \"gradcam_images.zip\")\n\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for fname in os.listdir(OUT_BASE):\n        full = os.path.join(OUT_BASE, fname)\n        if os.path.isfile(full) and fname.lower().endswith(\".jpg\"):\n            zf.write(full, arcname=fname)\n\nprint(f\"Created ZIP archive at {zip_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}