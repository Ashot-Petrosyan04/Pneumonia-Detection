{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839},{"sourceId":11692410,"sourceType":"datasetVersion","datasetId":7338816},{"sourceId":11794488,"sourceType":"datasetVersion","datasetId":7406204},{"sourceId":389175,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":320652,"modelId":341199},{"sourceId":389276,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":320727,"modelId":341304},{"sourceId":389311,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":320744,"modelId":341327}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as F\nimport cv2\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import binary_dilation, maximum_filter, minimum_filter\nfrom torchvision.models.inception import InceptionAux","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:13.939875Z","iopub.execute_input":"2025-05-13T13:08:13.940146Z","iopub.status.idle":"2025-05-13T13:08:13.945394Z","shell.execute_reply.started":"2025-05-13T13:08:13.940127Z","shell.execute_reply":"2025-05-13T13:08:13.944607Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:13.946653Z","iopub.execute_input":"2025-05-13T13:08:13.946851Z","iopub.status.idle":"2025-05-13T13:08:13.964460Z","shell.execute_reply.started":"2025-05-13T13:08:13.946836Z","shell.execute_reply":"2025-05-13T13:08:13.963694Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":154},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/processed-lungs/processed_lungs/labels.csv')\n\nnodule_df = df[df['label'] == 1]\nsecond_df = df[df['label'] == 0]\n\nprint(\"we print nodule \")\nprint(nodule_df)\nprint(\"now second one\")\nprint(second_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:13.965078Z","iopub.execute_input":"2025-05-13T13:08:13.965320Z","iopub.status.idle":"2025-05-13T13:08:14.046857Z","shell.execute_reply.started":"2025-05-13T13:08:13.965304Z","shell.execute_reply":"2025-05-13T13:08:14.046265Z"}},"outputs":[{"name":"stdout","text":"we print nodule \n                   filename  label\n0     00000004_000_lung.png      1\n1     00000008_002_lung.png      1\n2     00000013_025_lung.png      1\n3     00000017_000_lung.png      1\n4     00000021_000_lung.png      1\n...                     ...    ...\n6326  00030703_001_lung.png      1\n6327  00030715_000_lung.png      1\n6328  00030722_000_lung.png      1\n6329  00030726_000_lung.png      1\n6330  00030793_000_lung.png      1\n\n[6331 rows x 2 columns]\nnow second one\n                     filename  label\n6331    00000001_000_lung.png      0\n6332    00000001_001_lung.png      0\n6333    00000001_002_lung.png      0\n6334    00000002_000_lung.png      0\n6335    00000003_000_lung.png      0\n...                       ...    ...\n112111  00030801_001_lung.png      0\n112112  00030802_000_lung.png      0\n112113  00030803_000_lung.png      0\n112114  00030804_000_lung.png      0\n112115  00030805_000_lung.png      0\n\n[105785 rows x 2 columns]\n","output_type":"stream"}],"execution_count":155},{"cell_type":"code","source":"df_bbox = pd.read_csv('/kaggle/input/bbox-list-2017-csv/BBox_List_2017.csv')\n\ndf_bbox = df_bbox[df_bbox[\"Finding Label\"] == \"Nodule\"]\nprint(df_bbox)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.048107Z","iopub.execute_input":"2025-05-13T13:08:14.048323Z","iopub.status.idle":"2025-05-13T13:08:14.059758Z","shell.execute_reply.started":"2025-05-13T13:08:14.048306Z","shell.execute_reply":"2025-05-13T13:08:14.058926Z"}},"outputs":[{"name":"stdout","text":"          Image Index Finding Label     Bbox [x           y           w  \\\n668  00001688_000.png        Nodule  667.496296  276.317460  122.446561   \n669  00004547_003.png        Nodule  899.386243  690.251852   53.096296   \n670  00023078_000.png        Nodule  741.180952  687.001058   52.012698   \n671  00023068_003.png        Nodule  351.085714  747.682540   35.758730   \n672  00013911_000.png        Nodule  118.112169  483.284656   85.604233   \n..                ...           ...         ...         ...         ...   \n742  00013674_000.png        Nodule  210.217989  319.661376   58.514286   \n743  00013751_003.png        Nodule  676.165079  610.065608   89.938624   \n744  00010103_014.png        Nodule  343.500529  412.850794   41.176720   \n745  00011576_000.png        Nodule  206.967196  582.975661   46.594709   \n746  00030413_003.png        Nodule  255.729101  237.307937   88.855026   \n\n             h]  Unnamed: 6  Unnamed: 7  Unnamed: 8  \n668  150.620106         NaN         NaN         NaN  \n669   58.514286         NaN         NaN         NaN  \n670   48.761905         NaN         NaN         NaN  \n671   33.591534         NaN         NaN         NaN  \n672  101.858201         NaN         NaN         NaN  \n..          ...         ...         ...         ...  \n742   57.430688         NaN         NaN         NaN  \n743   89.938624         NaN         NaN         NaN  \n744   45.511111         NaN         NaN         NaN  \n745   41.176720         NaN         NaN         NaN  \n746   76.935450         NaN         NaN         NaN  \n\n[79 rows x 9 columns]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":156},{"cell_type":"code","source":"BATCH_SIZE = 8\nLR = 0.001\nEPOCHS = 10\n\nmain_dest_dir = '/kaggle/working/'\nsource_base_dir = '/kaggle/input/processed-lungs'\n\nclass SegmentedChestXRayDataset(Dataset):\n    def __init__(self, input_size):\n        self.image_paths = []\n        self.labels = []\n        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                     \n        self.transform_positive = transforms.Compose([\n            transforms.Resize(input_size),\n            transforms.ToTensor(),\n            self.normalize\n        ])\n        \n        self.transform_negative = transforms.Compose([\n            transforms.Resize(input_size),\n            transforms.ToTensor(),\n            self.normalize\n        ])\n        \n        source_folder = 'processed_lungs/nodule'\n        for _, row in df_bbox.iterrows():\n            image_filename = row['Image Index']\n            img_path = os.path.join(source_base_dir,\n                                    source_folder,\n                                    image_filename)\n            root, ext = os.path.splitext(img_path)\n            new_path = f\"{root}_lung{ext}\"\n            self.image_paths.append(new_path)\n\n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        image = Image.open(img_path).convert('RGB')\n        \n        image = self.transform_positive(image)\n            \n        return image, img_path\n\n    def __len__(self):\n        return len(self.image_paths)\n        \n    def tackle_idxs(self, idxs):\n        image_paths_temp = []\n        labels_temp = []\n        \n        for i in idxs:\n            label = self.labels[i]\n            img_path = self.image_paths[i]\n            \n            image_paths_temp.append(img_path)\n            labels_temp.append(label)\n        \n        combined = list(zip(image_paths_temp, labels_temp))\n        random.shuffle(combined)\n        self.image_paths, self.labels = map(list, zip(*combined))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.060750Z","iopub.execute_input":"2025-05-13T13:08:14.060985Z","iopub.status.idle":"2025-05-13T13:08:14.070106Z","shell.execute_reply.started":"2025-05-13T13:08:14.060964Z","shell.execute_reply":"2025-05-13T13:08:14.069489Z"}},"outputs":[],"execution_count":157},{"cell_type":"code","source":"source_base_dir_data = '/kaggle/input/data/'\n\nfolder_ranges = [\n    (1335, 6, 'images_001'),\n    (3923, 13, 'images_002'),\n    (6585, 6, 'images_003'),\n    (9232, 3, 'images_004'),\n    (11558, 7, 'images_005'),\n    (13774, 25, 'images_006'),\n    (16051, 9, 'images_007'),\n    (18387, 34, 'images_008'),\n    (20945, 49, 'images_009'),\n    (24717, 0, 'images_010'),\n    (28173, 2, 'images_011'),\n    (30805, 0, 'images_012')\n]\n\nclass ChestXRayDataset(Dataset):\n    def __init__(self):\n        self.image_paths = []\n        self.labels = []\n        \n        for _, row in df_bbox.iterrows():\n            image_filename = row['Image Index']\n            x_cord = row['Bbox [x']\n            y_cord = row['y']\n            width_w = row['w']\n            height_h = row['h]']\n            base_name = os.path.splitext(image_filename)[0]\n            part1_str, part2_str = base_name.split('_')\n            part1, part2 = int(part1_str), int(part2_str)\n    \n            source_folder = None\n            for f_part1, f_part2, f_name in folder_ranges:\n                if part1 < f_part1 or (part1 == f_part1 and part2 <= f_part2):\n                    source_folder = f_name\n                    break\n            if not source_folder:\n                continue\n    \n            img_path = os.path.join(source_base_dir_data,\n                                    source_folder,\n                                    'images',\n                                    image_filename)\n    \n            self.image_paths.append(img_path)\n            self.labels.append([x_cord, y_cord, width_w, height_h])\n\n    def __getitem__(self, index):\n        img_path = self.image_paths[index]\n        label = self.labels[index]\n        image = Image.open(img_path).convert('RGB')\n        \n        return label, img_path\n\n    def __len__(self):\n        return len(self.image_paths)\n        \n    def tackle_idxs(self, idxs):\n        image_paths_temp = []\n        labels_temp = []\n        \n        for i in idxs:\n            label = self.labels[i]\n            img_path = self.image_paths[i]\n            \n            image_paths_temp.append(img_path)\n            labels_temp.append(label)\n        \n        combined = list(zip(image_paths_temp, labels_temp))\n        random.shuffle(combined)\n        self.image_paths, self.labels = map(list, zip(*combined))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.070773Z","iopub.execute_input":"2025-05-13T13:08:14.070960Z","iopub.status.idle":"2025-05-13T13:08:14.093546Z","shell.execute_reply.started":"2025-05-13T13:08:14.070945Z","shell.execute_reply":"2025-05-13T13:08:14.092851Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"class DenseNet121(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.densenet = torchvision.models.densenet121(weights=\"IMAGENET1K_V1\")\n\n        num_features = self.densenet.classifier.in_features\n        #self.densenet.classifier = nn.Linear(num_features, 1)\n        \n        self.densenet.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_features, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1)\n        )\n        \n        for param in self.densenet.parameters():\n            param.requires_grad = False\n\n        to_unfreeze = [\n            \"features.denseblock2\",\n            \"features.transition2\",\n            \"features.denseblock3\",\n            \"features.transition3\",\n            \"features.denseblock4\",\n            \"features.norm5\",\n            \"classifier\",\n        ]\n        \n        for name, param in self.densenet.named_parameters():\n            if any(name.startswith(layer) for layer in to_unfreeze):\n                param.requires_grad = True\n    \n    def forward(self, x):\n        return self.densenet(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.095415Z","iopub.execute_input":"2025-05-13T13:08:14.095639Z","iopub.status.idle":"2025-05-13T13:08:14.113809Z","shell.execute_reply.started":"2025-05-13T13:08:14.095624Z","shell.execute_reply":"2025-05-13T13:08:14.113162Z"}},"outputs":[],"execution_count":159},{"cell_type":"code","source":"class DenseNet121GradCam(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n\n        self.densenetGradCam = model\n        self.features_conv = model.densenet.features[:11]\n        # print(self.densenetGradCam)\n        # print(self.features_conv)\n        self.batch_norm = model.densenet.features.norm5\n        self.avgpool_head = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.classifier = model.densenet.classifier\n        self.sigmoid = nn.Sigmoid()\n        self.gradients = None\n\n    def activation_hook(self, grad):\n        self.gradients = grad\n \n    def forward(self, x):\n        x = self.features_conv(x)\n        h = x.register_hook(self.activation_hook)\n        x = self.batch_norm(x)\n        x = self.avgpool_head(x)\n        x = self.flatten(x)\n        x = self.classifier(x)\n        \n        return x\n    \n    def get_activation_gradients(self):\n        return self.gradients\n    \n    def get_activations(self, x):\n        return self.features_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.114670Z","iopub.execute_input":"2025-05-13T13:08:14.114867Z","iopub.status.idle":"2025-05-13T13:08:14.128607Z","shell.execute_reply.started":"2025-05-13T13:08:14.114852Z","shell.execute_reply":"2025-05-13T13:08:14.128006Z"}},"outputs":[],"execution_count":160},{"cell_type":"code","source":"class Resnet34(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.resnet34 = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n\n        for param in self.resnet34.parameters():\n            param.requires_grad = False\n\n        to_unfreeze = ['layer3', 'layer4', 'fc']\n        for name, param in self.resnet34.named_parameters():\n            if any(name.startswith(layer) for layer in to_unfreeze):\n                param.requires_grad = True\n\n        num_ftrs = self.resnet34.fc.in_features\n        self.resnet34.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_ftrs, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, x):\n        return self.resnet34(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.129273Z","iopub.execute_input":"2025-05-13T13:08:14.129491Z","iopub.status.idle":"2025-05-13T13:08:14.147710Z","shell.execute_reply.started":"2025-05-13T13:08:14.129468Z","shell.execute_reply":"2025-05-13T13:08:14.146895Z"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":"class Resnet34GradCam(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n\n        self.resnetGradCam = model\n        self.features_conv = nn.Sequential(*list(model.resnet34.children())[:-2])\n\n        # print(self.resnetGradCam)\n        # print(self.features_conv)\n        \n        self.avgpool_head = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.classifier = model.resnet34.fc\n        self.sigmoid = nn.Sigmoid()\n        self.gradients = None\n\n    def activation_hook(self, grad):\n        self.gradients = grad\n \n    def forward(self, x):\n        x = self.features_conv(x)\n        h = x.register_hook(self.activation_hook)\n        \n        x = self.avgpool_head(x)\n        x = self.flatten(x)\n        x = self.classifier(x)\n        \n        return x\n    \n    def get_activation_gradients(self):\n        return self.gradients\n    \n    def get_activations(self, x):\n        return self.features_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.148372Z","iopub.execute_input":"2025-05-13T13:08:14.148607Z","iopub.status.idle":"2025-05-13T13:08:14.162162Z","shell.execute_reply.started":"2025-05-13T13:08:14.148588Z","shell.execute_reply":"2025-05-13T13:08:14.161427Z"}},"outputs":[],"execution_count":162},{"cell_type":"code","source":"class Inception_V3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.inception_v3 = torchvision.models.inception_v3(pretrained=True)\n        \n        num_features = self.inception_v3.fc.in_features\n        self.inception_v3.fc = nn.Linear(num_features, 1)\n        \n        if self.inception_v3.aux_logits:\n            num_features_aux = self.inception_v3.AuxLogits.fc.in_features\n            self.inception_v3.AuxLogits.fc = nn.Linear(num_features_aux, 1)\n\n        for name, param in self.inception_v3.named_parameters():\n            param.requires_grad = False\n                \n        for name, param in self.inception_v3.named_parameters():\n            if name.startswith('Mixed_7c') or name.startswith('fc') or name.startswith('AuxLogits.fc'):\n                param.requires_grad = True\n    \n    def forward(self, x):\n        return self.inception_v3(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.163011Z","iopub.execute_input":"2025-05-13T13:08:14.163263Z","iopub.status.idle":"2025-05-13T13:08:14.185035Z","shell.execute_reply.started":"2025-05-13T13:08:14.163242Z","shell.execute_reply":"2025-05-13T13:08:14.184311Z"}},"outputs":[],"execution_count":163},{"cell_type":"code","source":"class Inception_V3GradCam(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n\n        self.inceptionGradCam = model\n        self.features_conv = nn.Sequential(\n            *[\n              m for m in model.inception_v3.children()\n              if not isinstance(m, InceptionAux)\n            ][:-3]\n        )\n\n        print(self.inceptionGradCam)\n        print(self.features_conv)\n        \n        self.avgpool_head = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self.dropout = model.inception_v3.dropout\n        self.classifier = model.inception_v3.fc\n        self.sigmoid = nn.Sigmoid()\n        self.gradients = None\n\n    def activation_hook(self, grad):\n        self.gradients = grad\n \n    def forward(self, x):\n        x = self.features_conv(x)\n        h = x.register_hook(self.activation_hook)\n        x = self.avgpool_head(x)\n        x = self.flatten(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        \n        return x\n    \n    def get_activation_gradients(self):\n        return self.gradients\n    \n    def get_activations(self, x):\n        return self.features_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.185808Z","iopub.execute_input":"2025-05-13T13:08:14.186022Z","iopub.status.idle":"2025-05-13T13:08:14.201094Z","shell.execute_reply.started":"2025-05-13T13:08:14.186004Z","shell.execute_reply":"2025-05-13T13:08:14.200387Z"}},"outputs":[],"execution_count":164},{"cell_type":"code","source":"def main():\n    test_dataset_Inception_V3 = SegmentedChestXRayDataset(299)\n    test_loader_Inception_V3 = DataLoader(test_dataset_Inception_V3, batch_size=1, shuffle=False)\n    \n    test_dataset = SegmentedChestXRayDataset(224)\n    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n    \n    test_dataset_bbox = ChestXRayDataset()\n    test_loader_bbox = DataLoader(test_dataset_bbox, batch_size=1, shuffle=False)\n    \n    model_DenseNet121 = DenseNet121().to(device)\n    model_DenseNet121.load_state_dict(torch.load(\"/kaggle/input/densenet121_final_model.pth/pytorch/default/1/DenseNet121_final_model.pth\", map_location=device))\n    model_DenseNet121.eval()\n    \n    gradcam_DenseNet121 = DenseNet121GradCam(model_DenseNet121)\n    gradcam_DenseNet121.eval()\n    \n    model_Resnet34 = Resnet34().to(device)\n    model_Resnet34.load_state_dict(torch.load(\"/kaggle/input/resnet34_final_model.pth/pytorch/default/1/Resnet34_final_model.pth\", map_location=device))\n    model_Resnet34.eval()\n    \n    gradcam_Resnet34 = Resnet34GradCam(model_Resnet34)\n    gradcam_Resnet34.eval()\n    \n    model_Inception_V3 = Inception_V3().to(device)\n    model_Inception_V3.load_state_dict(torch.load(\"/kaggle/input/inception_v3_final_model.pth/pytorch/default/1/Inception_V3_final_model.pth\", map_location=device))\n    model_Inception_V3.eval()\n    \n    gradcam_Inception_V3 = Inception_V3GradCam(model_Inception_V3)\n    gradcam_Inception_V3.eval()\n    \n    return\n    for idx, ((image_Inception_V3, img_path_Inception_V3), (image_Resnet34_DenseNet121, img_path_Resnet34_DenseNet121), (label_bbox, img_path_bbox)) in enumerate(zip(test_loader_Inception_V3, test_loader, test_loader_bbox)):\n        print(idx)\n        \n        image_Resnet34_DenseNet121 = image_Resnet34_DenseNet121.to(device)\n        image_Inception_V3 = image_Inception_V3.to(device)\n        image_Resnet34_DenseNet121.requires_grad = True\n        image_Inception_V3.requires_grad = True\n        \n        for name_of_gradcam, gradcam, image, img_path in [(\"gradcam_DenseNet121\", gradcam_DenseNet121, image_Resnet34_DenseNet121, img_path_Resnet34_DenseNet121), (\"gradcam_Resnet34\", gradcam_Resnet34, image_Resnet34_DenseNet121, img_path_Resnet34_DenseNet121), (\"gradcam_Inception_V3\", gradcam_Inception_V3, image_Inception_V3, img_path_Inception_V3)]:\n            gradcam.zero_grad()\n            logits = gradcam(image)\n            logits[0, 0].backward()\n    \n            activations = gradcam.get_activations(image).cpu().detach()\n            gradients = gradcam.get_activation_gradients().cpu()\n            pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n    \n            for i in range(activations.shape[1]):\n                activations[:, i, :, :] *= pooled_gradients[i]\n    \n            heatmap = torch.sum(activations, dim=1).squeeze()\n            heatmap = torch.clamp(heatmap, min=0)\n            heatmap /= torch.max(heatmap)\n            heatmap = heatmap.numpy()\n    \n            image_original = cv2.imread(img_path[0])\n            heatmap_resized = cv2.resize(heatmap, (image_original.shape[1], image_original.shape[0]))\n            heatmap_resized_uint8 = np.uint8(255 * heatmap_resized)\n            heatmap_color = cv2.applyColorMap(heatmap_resized_uint8, cv2.COLORMAP_JET)\n            superimposed_img = heatmap_color * 0.4 + image_original\n            superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n    \n            neighborhood_size = 15\n            threshold_val = 0.1\n    \n            heatmap_norm = heatmap_resized_uint8.copy()\n    \n            data_max = maximum_filter(heatmap_norm, size=neighborhood_size)\n            maxima = (heatmap_norm == data_max)\n            data_min = minimum_filter(heatmap_norm, size=neighborhood_size)\n            diff = ((data_max - data_min) > threshold_val)\n            maxima[diff == 0] = 0\n    \n            for _ in range(5):\n                maxima = binary_dilation(maxima)\n    \n            labeled, num_objects = ndimage.label(maxima)\n            centers = ndimage.center_of_mass(heatmap_norm, labeled, range(1, num_objects+1))\n    \n            scale_factor = image_original.shape[1] / 224\n            w_k = int(122 * scale_factor)\n            h_k = int(132 * scale_factor)\n            \n            for center in centers:\n                y, x = center\n                if heatmap_norm[int(y), int(x)] > np.max(heatmap_norm) * 0.9:\n                    left = int(max(x - w_k/2, 0))\n                    upper = int(max(y - h_k/2, 0))\n                    right = int(min(left + w_k, image_original.shape[1]))\n                    lower = int(min(upper + h_k, image_original.shape[0]))\n                    left = int(left)\n                    right = int(right)\n                    upper = int(upper)\n                    lower = int(lower)\n    \n                    cv2.rectangle(superimposed_img, (left, upper), (right, lower), (0, 255, 0), 2)\n    \n            cv2.imwrite(f\"{name_of_gradcam}_{idx}.jpg\", superimposed_img)\n\n        original_img_bbox = cv2.imread(img_path_bbox[0])\n        if original_img_bbox is not None:\n            x, y, w, h = label_bbox\n            x = int(x.item())\n            y = int(y.item())\n            w = int(w.item())\n            h = int(h.item())\n            print(x)\n            print(y)\n            print(w)\n            print(h)\n            cv2.rectangle(original_img_bbox, (x, y), (x + w, y + h), (0, 255, 0), 2)\n            \n            cv2.imwrite(f\"orig_{idx}.jpg\", original_img_bbox)\n        else:\n            print(f\"Failed to load image: {img_path_bbox[0]}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:14.203007Z","iopub.execute_input":"2025-05-13T13:08:14.203554Z","iopub.status.idle":"2025-05-13T13:08:15.502275Z","shell.execute_reply.started":"2025-05-13T13:08:14.203533Z","shell.execute_reply":"2025-05-13T13:08:15.501373Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/153729396.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_DenseNet121.load_state_dict(torch.load(\"/kaggle/input/densenet121_final_model.pth/pytorch/default/1/DenseNet121_final_model.pth\", map_location=device))\n/tmp/ipykernel_31/153729396.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_Resnet34.load_state_dict(torch.load(\"/kaggle/input/resnet34_final_model.pth/pytorch/default/1/Resnet34_final_model.pth\", map_location=device))\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"[]\n['inception_v3.AuxLogits.conv0.conv.weight', 'inception_v3.AuxLogits.conv0.bn.weight', 'inception_v3.AuxLogits.conv0.bn.bias', 'inception_v3.AuxLogits.conv0.bn.running_mean', 'inception_v3.AuxLogits.conv0.bn.running_var', 'inception_v3.AuxLogits.conv0.bn.num_batches_tracked', 'inception_v3.AuxLogits.conv1.conv.weight', 'inception_v3.AuxLogits.conv1.bn.weight', 'inception_v3.AuxLogits.conv1.bn.bias', 'inception_v3.AuxLogits.conv1.bn.running_mean', 'inception_v3.AuxLogits.conv1.bn.running_var', 'inception_v3.AuxLogits.conv1.bn.num_batches_tracked', 'inception_v3.AuxLogits.fc.weight', 'inception_v3.AuxLogits.fc.bias']\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/153729396.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_dict = torch.load('/kaggle/input/inception_v3_final_model.pth/pytorch/default/1/Inception_V3_final_model.pth', map_location=device)\n","output_type":"stream"}],"execution_count":165},{"cell_type":"code","source":"import os\nimport zipfile\n\nOUT_BASE = \"/kaggle/working/\"\nzip_path = os.path.join(OUT_BASE, \"gradcam_images.zip\")\n\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for fname in os.listdir(OUT_BASE):\n        full = os.path.join(OUT_BASE, fname)\n        if os.path.isfile(full) and fname.lower().endswith(\".jpg\"):\n            zf.write(full, arcname=fname)\n\nprint(f\"Created ZIP archive at {zip_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:08:15.503275Z","iopub.execute_input":"2025-05-13T13:08:15.503567Z","iopub.status.idle":"2025-05-13T13:08:16.344180Z","shell.execute_reply.started":"2025-05-13T13:08:15.503543Z","shell.execute_reply":"2025-05-13T13:08:16.343435Z"}},"outputs":[{"name":"stdout","text":"Created ZIP archive at /kaggle/working/gradcam_images.zip\n","output_type":"stream"}],"execution_count":166}]}